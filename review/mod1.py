# -*- coding: utf-8 -*-
"""MOD1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z5uudo6dizOY1O8WdZIvSxF_eygam4_d

# Cherries and Grapes
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

data = {
    'y': [1.18, 1.06, 1.08, 1.06, 1.05, 1.12, 1.14, 1.18, 1.18, 1.16,
          1.14, 1.12, 1.19, 1.20, 1.20, 1.22, 1.20, 1.21, 1.15, 1.10, 1.12],
    'x': [3.7, 3.2, 3.3, 3.4, 2.9, 3.5, 3.4, 3.6, 3.7, 3.5,
          3.6, 3.7, 3.75, 3.9, 3.6, 4.3, 4.1, 4.15, 3.8, 3.4, 3.2],
    'label': ['b', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b',
              'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'a', 'a']
}

df = pd.DataFrame(data)

df['fruit'] = df['label'].map({'a': 'grape', 'b': 'cherry'})
df['label_num'] = df['label'].map({'a': 0, 'b': 1})

"""##Quick EDA"""

df['fruit'].value_counts()

df[['x','y','fruit']].groupby('fruit').describe()

"""## Adding points to plot"""

def add_point(x,y,label, col = 'blue'):
  plt.scatter(x, y, color=col, s=100)
  plt.text(x, y, label, color='black')

import seaborn as sns
plt.figure(figsize=(16, 3))
sns.scatterplot(data=df, x='x', y='y', hue='fruit', palette = ["red", "purple"], s=100)
plt.xlabel('x (circumference)')
plt.ylabel('y (height)')

#add point P
add_point(3.52, 1.19, "  P", col = "black")
#add point Q
add_point(3.48, 1.175, "  Q", col = "goldenrod")
#add point R
add_point(3.5, 1.135, "  R", col = "pink")

"""##Exercise: Points P, Q, R

For each of the points above:
Which 3 data points are closest? Are they majority grapes or cherries?
Based on those three neighbors, would you classify the point as a grape or a cherry?

##Exercise: Decision Boundaries

Suppose instead of using the 3 nearest neighbors, we use a different $k$.

For each $k$:
* Describe what would happen to the region that would be shaded to classify cherries.
* Would this be better or worse than 3-NN? Explain

1. $k$ = 1
2. $k$ = 20
"""

plt.figure(figsize=(16, 3))
sns.scatterplot(data=df, x='x', y='y', hue='fruit', palette = ["red", "purple"], s=100)
plt.xlabel('x (circumference)')
plt.ylabel('y (height)')

"""1.


2.

#PCA

## Regression Line
"""

df['ID'] = range(1, len(df) + 1)
plt.figure(figsize=(16, 3))
sns.regplot(x=df['x'], y=df['y'], color = "black", scatter_kws={'s': 100}, line_kws={'color': 'tab:olive'}, ci=None)
for i, row in df.iterrows():
    plt.text(row['x'], row['y'], str(row['ID']), fontsize=10,
             ha='right', va='bottom', color='black', weight='bold')
plt.xlabel('x (circumference)')
plt.ylabel('y (height)')

"""Most of the variance is happening along a similar line to the green line, where equal parts $x$ and $y$ are contributing to the variance.

If you run PCA with two components on this dataset, you'll find that >90% of the variance of the data can be attributed to this.

##PCA transformations

Below are two plots. The first shows how we can use PCA to reduce dimensions from 2 down to 1. We then run a regression line and predict y based off that PCA1. Notice that our axes are PC1 and y. If we overlay the point IDs you can see where some of these points ended up.
"""

import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression



scaler = StandardScaler()
X_scaled = scaler.fit_transform(df[['x', 'y']])

# Apply PCA
pca = PCA(n_components=1)
X_pca = pca.fit_transform(X_scaled)
df['X_pca'] = X_pca

# Use PC1 as input for Linear Regression
reg = LinearRegression()
reg.fit(X_pca, df['y'])  # Predicting 'y' using PC1

# Predictions
y_pred = reg.predict(X_pca)

plt.figure(figsize=(16, 3))
sns.scatterplot(data = df, x = 'X_pca', y='y', hue=df['fruit'], palette={'grape': 'purple', 'cherry': 'red'}, s=100, edgecolor="black")
#plt.scatter(X_pca, df['y'], label="Original Data", color = "black")
plt.plot(X_pca, y_pred, color='orange', label="Regression on PC1")
plt.xlabel("PC1")
plt.ylabel("y")
plt.legend()
plt.title("Linear Regression using PC1")
for i, row in df.iterrows():
    plt.text(row['X_pca'], row['y'], str(row['ID']), fontsize=10,
             ha='right', va='bottom', color='black', weight='bold')
plt.show()

"""Here's where we lose visability. If we try to see what this looks like on orgininal axes, we lose lineararity."""

plt.figure(figsize=(16, 3))
sns.scatterplot(data = df, x = 'x', y='y', hue=df['fruit'], palette={'grape': 'purple', 'cherry': 'red'}, s=100, edgecolor="black")
#plt.scatter(X_pca, df['y'], label="Original Data", color = "black")
plt.plot(df['x'], y_pred, color='orange', label="Regression on PC1")
for i, row in df.iterrows():
    plt.text(row['x'], row['y'], str(row['ID']), fontsize=10,
             ha='right', va='bottom', color='black', weight='bold')
plt.xlabel("x")
plt.ylabel("y")
plt.legend()
plt.title("Linear Regression using PC1")
plt.show()

"""#Logistic Regression

Similar to linear regression since it uses gradient descent but with few changes:
* Instead of a straight-line equation, it uses a probabilistic equation
* Instead of MSE it uses log-loss
* Additionally, it â€œsquishedâ€ the probability it towards 0 or 1 using a function (usually sigmoid)

So it uses gradient descent but with a different cost function and adds a "squish" effect

## Classifying a new point P

Suppose we have a new point, P,  we want to classify. P = (3.52, 1.19). Let's see if this algo classifies P as a cherry or a grape.

Recall, 3-NN classified P as a cherry.
"""

P = (3.52, 1.19)

"""##Init coefficients

Like with linear regression we have coefficients we need to optimize. In logistic regression these coefficients are often referred to as "weights" or $w_i$.

We initialize these coefficients randomly, then use gradient descent to update until they minimize our cost function.

Let's set our weights $w_0, w_1, w_2$ to the following:
"""

import random
w_0 = -0.1
w_1 = 0.3
w_2 = 0.5

"""##Logit
Now we create a logit function: $z = w_1*x_1 + w_2*x_2 + w_0$ where $x_1$ refers to $x$ (circumference of fruit) and $x_2$ refers to y (height of fruit).

"""

x_1 = P[0] #x
x_2 = P[1] #y

z = w_1*x_1 + w_2*x_2 + w_0
z

"""## Squish with Sigmoid

We're now going to squish this with the sigmoid function $\sigma(z) = \frac{1}{1 + e^{-z}}$

"""

import numpy as np

sigmoid = 1 / (1 + np.exp(-z))
round(float(sigmoid),5)

"""## Threshold

The output $\sigma(z) $ is very close to 1, meaning the model strongly predicts that point P is a cherry (class 1).
"""

if sigmoid > 0.5:
  print(" ğŸ’ Cherry ğŸ’")
else:
  print("ğŸ‡ Grape ğŸ‡")

"""#Exercise: Update weights

Now, go back and experiment with different initial weights. Observe how they affect the sigmoid function output and the classification.

* Set all initial weights to zero (naive init approach)
* Set all initial weights to large negative values (e.g., -10)
* Set initial weights to small random values (recommended init approach)
"""