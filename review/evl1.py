# -*- coding: utf-8 -*-
"""EVL1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gp1jEQmcIjyLnxeP-6uq6fxSazP2XVvN

#Exercise: terminology check

1. How would you describe bias and variance to a non-technical person?

2. How would you explain what the difference between a simple and complex model is?

# Example with Titantic

We're going to use the titantic dataset to see how well age, sex, and pclass can predict if an individual survied or not.

Since we have a target, this will a supervised approached. And since this is predicting a value it is a regression problem. Now we'll get into regression in a few weeks, but since survived is binary (yes or no, 1 or 0) we will be doing what is called logistic regression.

## Prepare the data
The following code will prepare our feature set `X` and our target variable `y` for a logisitic regression algorithm.
"""

import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import numpy as np

# Load Titanic dataset
titanic = sns.load_dataset('titanic')

# Drop rows with missing values for simplicity
titanic.dropna(subset=['age', 'sex', 'pclass', 'survived'], inplace=True)

# Features and target
X = titanic[['age', 'sex', 'pclass']]
y = titanic['survived']

# Convert categorical data to binary
X.loc[X['sex'] == 'male', 'sex'] = 1
X.loc[X['sex'] == 'female', 'sex'] = 0

#model selection
model = LogisticRegression(max_iter=1000)

"""## One random model
Let's see how to run this model a single time. Run the code chunk below. Then change the `random_state` to a few different numbers. What did you notice?

Change the `random_state` to None and run a few times. What did you notice?
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
score = accuracy_score(y_test, y_pred)
print(f"One model Accuracy: {score:.4f}")

"""random_state observations:

# Overfitting
We'd like to see how the different solutions to fix the overfitting problem are set up.

##Solution 1: repeat k times
Below is the first solution: repeat the model build 5 times, and then average all accuracy metrics.
"""

# Repeated 5 times with different random splits
repeated_scores = []
random = [23,34,2,1,3]
for _ in range(5):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random[_])
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    score = accuracy_score(y_test, y_pred)
    repeated_scores.append(score)

print(f"Average score after 5 repetitions: {np.mean(repeated_scores):.4f}")

"""## Solution 2: k-fold CV

Below is the second solution: randomly shuffle data into k-folds where you train on 1,2,3,4 then test on 5; train on 2,3,4,5 test on 1; ect.
"""

from sklearn.model_selection import cross_val_score

# Step 2: Apply K-Fold Cross-Validation (using 5 folds)
cv_scores = cross_val_score(model, X, y, cv=5)

print(f"Cross-Validation Scores: {cv_scores}")
print(f"Mean Cross-Validation Accuracy: {cv_scores.mean():.4f}")

"""##Solution 3: stratified k-fold CV

This ensures our target variable is equally distributed into test and train sets.
"""

from sklearn.model_selection import StratifiedKFold

# Stratified K-Fold Cross-Validation
stratified_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
stratified_scores = []

for train_index, test_index in stratified_kf.split(X, y):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    score = accuracy_score(y_test, y_pred)
    stratified_scores.append(score)

print(f"Stratified K-Fold Cross-Validation Scores: {stratified_scores}")
print(f"Mean Accuracy from Stratified K-Fold CV: {np.mean(stratified_scores):.4f}")