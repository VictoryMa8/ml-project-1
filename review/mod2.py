# -*- coding: utf-8 -*-
"""MOD2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nn_nZ9LdjUNrpjgiB4h0MInTKX1jk5Tw

# Cherries and Grapes
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns #add to your code

data = {
    'height': [1.18, 1.06, 1.08, 1.06, 1.05, 1.12, 1.14, 1.18, 1.18, 1.16,
          1.14, 1.12, 1.19, 1.20, 1.20, 1.22, 1.20, 1.21, 1.15, 1.10, 1.12],
    'circumference': [3.7, 3.2, 3.3, 3.4, 2.9, 3.5, 3.4, 3.6, 3.7, 3.5,
          3.6, 3.7, 3.75, 3.9, 3.6, 4.3, 4.1, 4.15, 3.8, 3.4, 3.2],
    'label': ['b', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b',
              'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'a', 'a']
}

df = pd.DataFrame(data)

df['fruit'] = df['label'].map({'a': 'grape', 'b': 'cherry'})
df['label_num'] = df['label'].map({'a': 0, 'b': 1})

plt.figure(figsize=(16, 3))
new_sample = {'height': 1.15, 'circumference': 3.5} #add to your code

# Scatter plot of original data
sns.scatterplot(data=df, x='circumference', y='height', hue='fruit', palette={'grape': 'purple', 'cherry': 'red'}, s=100)

# Plot the new sample
plt.scatter(new_sample['circumference'], new_sample['height'], color='tab:olive', s=150, edgecolors='black', label='New Sample')

# Labels and title
plt.xlabel("Circumference")
plt.ylabel("Height")
plt.legend()

"""#Naive Bayes


"""

df['fruit'].value_counts()

"""##Feature stats

Now, let's take a look at the mean and standard deviation are for each of these categories.
"""

stats = df[['circumference','height', 'fruit']].groupby('fruit').describe()
stats

"""Let's put this information into a dictionary."""

fruit_stats = {
    fruit: {
        'height': {'mean': stats.loc[fruit, ('height', 'mean')], 'std': stats.loc[fruit, ('height', 'std')]},
        'circumference': {'mean': stats.loc[fruit, ('circumference', 'mean')], 'std': stats.loc[fruit, ('circumference', 'std')]}
    }
    for fruit in stats.index
}

"""##Compute Priors $P(Y)$

Calculate the fraction of grapes and cherries in the dataset.
"""

prior_grape = len(df[df['fruit'] == 'grape']) / len(df)
prior_cherry = len(df[df['fruit'] == 'cherry']) / len(df)

print(f"Probability of grapes is {round(prior_grape,3)},\nand the probability of cherries is {round(prior_cherry,3)}")

"""##New record

"""

new_sample = {'height': 1.15, 'circumference': 3.5}

"""## Compute Likelihood $P(Xâˆ£Y)$

Use a Gaussian distribution to estimate the probability of the new sample's height and circumference for each class.
"""

from scipy.stats import norm

def gaussian_prob(x, stats, class_type, feature):
  data = stats[class_type][feature]
  return norm.pdf(x, data['mean'], np.sqrt(data['std']))

likelihood_grape = gaussian_prob(new_sample['height'], fruit_stats, 'grape', 'height') * \
                   gaussian_prob(new_sample['circumference'], fruit_stats, 'grape', 'circumference')

likelihood_cherry = gaussian_prob(new_sample['height'], fruit_stats, 'cherry', 'height') * \
                    gaussian_prob(new_sample['circumference'], fruit_stats, 'cherry', 'circumference')

"""##Compute Evidence $P(X)$

Sum of the weighted likelihoods for both classes.
"""

evidence = (likelihood_grape * prior_grape) + (likelihood_cherry * prior_cherry)
evidence

"""##Compute Posteriors $P(Yâˆ£X)$

Use Bayes' theorem to determine the probability of the fruit being a grape or cherry.
"""

posterior_grape = (likelihood_grape * prior_grape) / evidence
posterior_cherry = (likelihood_cherry * prior_cherry) / evidence

"""##Predict the Class

Choose the class with the highest posterior probability.
"""

print(f"P(Y=grape | X) = {posterior_grape:.4f}")
print(f"P(Y=cherry | X) = {posterior_cherry:.4f}")
predicted_class = 'grape' if posterior_grape > posterior_cherry else 'cherry'
print(f"Predicted class: {predicted_class}")

"""#Support Vector Machine SVM

Let's use some new data so that the boundaries between two classes is a bit easier to see.
"""

points = {'id': ['p_0', 'p_1', 'p_2', 'p_3', 'p_4', 'p_5', 'p_6', 'p_7'],
          'x': [-2, 0, -1,  0,  2,
                1,  1,  0],
          'y': [ 0, -1, -1,  2,  1.5,
                0.5,  0,  0],
          'label': ['A', 'A', 'A', 'B', 'B', 'B', 'B', 'B']
}

"""## Adding separating line

We'll create a function to draw a line on the plot of our data
"""

def add_line(slope, intercept,coloring="pink"):
  x_line = np.linspace(-3,2, 10)
  y_line = slope * x_line + intercept
  plt.plot(x_line, y_line, color=coloring, linewidth=2)

"""Run the code below to plot the data.

Then update the slope and intercept until you get a line that separates the data the best.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
sns.scatterplot(data=points, x='x', y='y',
                hue=points['label'],
                palette=["black", "tab:olive"],
                s=100)
plt.xlabel('x')
plt.ylabel('y')
plt.legend(loc='upper left')

#equation of the line
add_line( -1,-0.5, "pink")

"""## Deleting points

For the following points, describe if your separating line would change if these points were deleted.

1. black (-1,-1)

2. green (1,0.5)

3. black(-2,0)

4. green (0,0)


Any points that matter the most will affect the separating line. These are called the support vectors.

## Dividing Line

In order to describe our separating line, we'll use a linear function of two variables $x$ and $y$. This will look different than the separating line we created before, something like $f(x,y)=3x_1 + 2x_2 -1$. Since this has a different form, we'll call this the *dividing line*.

The dividing line should be the set of points $(x_1, x_2)$ such that $f(x_1,x_2)=0$. This is ideal since the two sides can be denoted where $f$ is positive and $f$ is negative. In other words, $f(x_1, x_2)>0$ and $f(x_1, x_2)<0$ will distinguish our two categories.

Follow these steps to find $f(x_1,x_2$)
1. Write down the equation of your line you found above in the form: $y=mx+b$

2. Change the $x$ to an $x_1$ and the $y$ to $x_2$

3. Subtract everything over to the right side of the equation, so the left side is just 0. The function $f$ is defined by the right side of the equation you wrote. In other words, you've just written what $f(x_1,x_2) $ is.

y = mx+b

y = -1x + b

x2 = -1x1 -0.5

f(x1,x2) = ...

#Classifying points
Update the function below so the coefficients reflect your dividing line $f(x_1,x_2)$
"""

def dividing_line(x1,x2):
  x1_slope = -0.5
  x2_slope = -1
  intercept = -0.5
  return x1*x1_slope +x2*x2_slope + intercept

"""Run the code chunks below. If you plug in coordinates for black dots, do you get positive or negative numbers back? If you plug in coordinates for green dots, do you get positive or negative numbers back?

"""

dividing_line(-1,-1) #black

dividing_line(1,0.5) #green

dividing_line(-2,0) #black

dividing_line(0,0) #green

"""## Hyperplanes

If all the black dots give you outputs with the same sign from $f(x_1,x_2)$, and all the green dots give you outputs with the oppositie sign, you have made your first *separating hyperplane*!

We use this idea of separating hyperplanes to create a classifier called a support vector machine.
* The support of a function is the set where the function isn't zzero. It's really about where the function $f$ is positive or negative, since that determines the classes.
* More specifically, the support vectors are the training data points which are closest to the separating hyperplane. In deleting points example, we saw that a few points are most important. In this context, we have a vector (sort of like a point) containing coordinates $(x_1,x_2)$

## sklearn example

Below is the code to implement
"""

from sklearn import svm
import matplotlib.pyplot as plt

df = pd.DataFrame(points)
df['label_num'] = df['label'].map({'A': 0, 'B': 1})
X = df[['x', 'y']]
y = df['label_num']


clf = svm.SVC(kernel='linear')
clf.fit(X, y)

# Create the scatter plot
sns.scatterplot(data=df, x='x', y='y', hue='label', palette=["black", "tab:olive"], s=100)

# Plot the decision boundary
xx, yy = np.meshgrid(np.linspace(df['x'].min() - 1, df['x'].max() + 1, 100),
                     np.linspace(df['y'].min() - 1, df['y'].max() + 1, 100))

# Plot the decision boundary and margins
plt.contourf(xx, yy, Z, alpha=0.2, cmap="viridis")
plt.contour(xx, yy, Z, levels=[0.5], linewidths=2, colors='black')

# Plot the margins (support vectors)
margin = 1 / np.sqrt(np.sum(clf.coef_ ** 2))  # The margin is the inverse of the SVM's weight magnitude
plt.contour(xx, yy, Z, levels=[0 + margin], linewidths=3, colors='darkgreen', linestyles='dashed')  # positive margin
plt.contour(xx, yy, Z, levels=[1 - margin], linewidths=3, colors='purple', linestyles='dashed')  # negative margin

# Add labels and title
plt.xlabel('x')
plt.ylabel('y')
plt.legend(loc='upper left')

# Show the plot
plt.show()

"""The optimal equation for the separating hyperplane is:"""

#coefficients of the hyperplane
coef = clf.coef_[0]
intercept = clf.intercept_[0]

#equation of the hyperplane:
f"f(x1,x2) = {round(coef[0],2)}*x1 + {round(coef[1],2)}*x2 + ({round(intercept,2)}) = 0"